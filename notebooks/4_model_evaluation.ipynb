{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602cdd25",
   "metadata": {},
   "source": [
    "# Lab 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274dde6",
   "metadata": {},
   "source": [
    "Properly evaluating your machine learning model is essential to assess it performance. Importantly, we want to model to be **generalizable**, i.e. it performs well on unseen (future) data.\n",
    "\n",
    "Typically, the data is split into a **train and test set**. The model is then trained on the former, and its performance is evaluated _in the end_ on the latter. It is _very important_ to only evaluate performance of your model on the test set as the very last step. In case the test performance is used to direct model changes, it is no longer independent and **overfitting** will occur.\n",
    "\n",
    "In case your model has **hyperparameters** that can be tuned, the data is split into a third set, called the **validation set**. The validation set can then be used to assess the performance for different hyperparameter values to keep the test set separate for the final performance evaluation.\n",
    "\n",
    "We will now use k-fold cross-validation to develop a random forest machine learning model (random forests will be explained in more detail tomorrow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data.\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 93, 2487\n",
    "X = pd.DataFrame(\n",
    "    np.random.normal(loc=5, scale=2, size=(n_samples, n_features)),\n",
    "    columns=[f\"feature_{i}\" for i in range(n_features)],\n",
    "    index=[f\"run_{i}.mzML\" for i in range(n_samples)]\n",
    ")\n",
    "y = np.random.choice(2, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef7257",
   "metadata": {},
   "source": [
    "The data corresponds to the simulated abundance of 2487 metabolite features that were measured in a large experiment consisting of 93 mass spectrometry runs. Additionally, each run corresponds to either a healthy (`0`) or diseased (`1`) patient.\n",
    "\n",
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c57b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f59fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).map({0: \"Healthy\", 1: \"Diseased\"}).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b1775",
   "metadata": {},
   "source": [
    "Great. We hope to create a diagnostic test that can predict disease state from our metabolite read-outs.\n",
    "\n",
    "Let's try to build a machine learning classifier that predicts whether a run corresponds to healthy or diseased.\n",
    "\n",
    "This is a typical molecular dataset in which there are much more features $k$ (i.e. measured metabolites) than samples $n$ (i.e. runs/patients). Most machine learning models can't directly deal with data in which $k >> n$.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><b>What is an approach that we have discussed today that can be used to reduce the number of features?</b> (Click to expand)\n",
    "</summary>\n",
    "\n",
    "PCA can be used for dimensionality reduction prior to training the machine learning model.\n",
    "</details>\n",
    "\n",
    "We will use **feature selection** to reduce the number of features to be used by the machine learning model. There exist various feature selection methods. Here we will use a statistical test (ANOVA) to determine whether features are associated with our target variable (disease status) and only use statistically significant features (p-value < 0.05) to build our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6436c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions.\n",
    "class FeatureSelector:\n",
    "    \n",
    "    def __init__(self, pval_threshold):\n",
    "        self.pval_threshold = pval_threshold\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        _, pval = f_classif(X, y)\n",
    "        self.X_mask = np.where(pval < self.pval_threshold)[0]\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X[:, self.X_mask]\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit(X, y).transform(X)\n",
    "    \n",
    "\n",
    "def plot_roc(tprs, aucs):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    mean_tpr, std_tpr = np.nanmean(tprs, axis=0), np.nanstd(tprs, axis=0)\n",
    "    mean_tpr[0], mean_tpr[-1] = 0, 1\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        label=f\"AUC = {np.mean(aucs):.3f} Â± {np.std(aucs):.3f}\",\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        mean_fpr,\n",
    "        mean_tpr - std_tpr,\n",
    "        mean_tpr + std_tpr,\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], c=\"black\", ls=\"--\")\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "\n",
    "    ax.legend(loc=\"lower right\", frameon=False)\n",
    "\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ceaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build our classifier!\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Only retain features that are statistically significant:\n",
    "pvalue_threshold = 0.05\n",
    "X_filtered = FeatureSelector(pvalue_threshold).fit_transform(X.values, y)\n",
    "\n",
    "# We evaluate the model using 5-fold cross-validation and record the\n",
    "# performance on the validation set:\n",
    "tprs, aucs = [], []\n",
    "mean_fpr = np.linspace(0, 1, 101)\n",
    "for train, test in StratifiedKFold(n_splits=5).split(X_filtered, y):\n",
    "    classifier.fit(X_filtered[train], y[train])\n",
    "    y_pred = classifier.predict(X_filtered[test])\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y[test], y_pred)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    aucs.append(roc_auc_score(y[test], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2445e3e",
   "metadata": {},
   "source": [
    "How did we do? We assess the model performance using an ROC curve.\n",
    "\n",
    "We show the average ROC curve over all 5 cross-validation folds. The shaded area indicates the standard deviation to assess the robustness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426a8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(tprs, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6d8c7",
   "metadata": {},
   "source": [
    "**What do we conclude from the ROC curve?** Are we able to predict disease state from our metabolic markers?\n",
    "<br><br>\n",
    "<details>\n",
    "<summary>(Click to expand)\n",
    "</summary>\n",
    "\n",
    "But wait! Scroll up to the second code cell. Our simulated data was _randomly generated_, so there should not be any signal in the data that allows us the predict disease state!\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><b>What did we do wrong?</b> (Click to expand)\n",
    "</summary>\n",
    "\n",
    "The problem is that we <i>performed feature selection outside of the cross-validation loop</i>.\n",
    "    \n",
    "Remember the **prime directive of machine learning**:\n",
    "\n",
    "> Only evaluate your models on data that has never been used for training.\n",
    "\n",
    "Performing feature selection on the _full dataset_ violates this rule, even though we subsequently use cross-validation to train and evaluate our model. This leads to **data leakage**, which is a subtle but common issue. In this case, because we used only statistically significant features for model training, class information _leaked_ to our validation set.\n",
    "    \n",
    "Instead, **feature selection needs to be part of the cross-validation set-up**. Let's try this again.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c971e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now feature selection is part of the model pipeline and will be performed\n",
    "# for each fold separately during cross-validation:\n",
    "classifier = make_pipeline(\n",
    "    FeatureSelector(pvalue_threshold),\n",
    "    RandomForestClassifier(),\n",
    ")\n",
    "\n",
    "# We perform 5-fold cross-validation again:\n",
    "tprs, aucs = [], []\n",
    "mean_fpr = np.linspace(0, 1, 101)\n",
    "for train, test in StratifiedKFold(n_splits=5).split(X, y):\n",
    "    classifier.fit(X.values[train], y[train])\n",
    "    y_pred = classifier.predict(X.values[test])\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y[test], y_pred)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    aucs.append(roc_auc_score(y[test], y_pred))\n",
    "    \n",
    "plot_roc(tprs, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de1e09",
   "metadata": {},
   "source": [
    "Note the huge difference in classifier performance! Now our model achieved virtually random performance. This is expected, as the input data was randomly generated, and thus no predictive performance is expected.\n",
    "\n",
    "**Be extremely careful for data leakage!** This can cause a significant optimistic bias during evaluation of your model performance, and such models might perform worse for unseen data. (Or worst case scenario, might not be better than a random coin flip, as in our example.)\n",
    "\n",
    "Let's try the same for non-random data to further investigate the effects of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0974687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a two-class dataset with the same number of features and samples,\n",
    "# but with only 20 informative features (other features are random):\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_redundant=0,\n",
    "    n_informative=20,\n",
    "    random_state=1,\n",
    "    n_clusters_per_class=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will train our model incorrectly (including data leakage):\n",
    "# Let's build our classifier!\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Only retain features that are statistically significant:\n",
    "pvalue_threshold = 0.05\n",
    "X_filtered = FeatureSelector(pvalue_threshold).fit_transform(X, y)\n",
    "\n",
    "# We evaluate the model using 5-fold cross-validation and record the\n",
    "# performance on the validation set:\n",
    "tprs, aucs = [], []\n",
    "mean_fpr = np.linspace(0, 1, 101)\n",
    "for train, test in StratifiedKFold(n_splits=5).split(X_filtered, y):\n",
    "    classifier.fit(X_filtered[train], y[train])\n",
    "    y_pred = classifier.predict(X_filtered[test])\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y[test], y_pred)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    aucs.append(roc_auc_score(y[test], y_pred))\n",
    "    \n",
    "plot_roc(tprs, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd921bbf",
   "metadata": {},
   "source": [
    "We get excellent performance again! Let's see what happens when we perform feature selection _inside_ of the cross-validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now feature selection is part of the model pipeline and will be performed\n",
    "# for each fold separately during cross-validation:\n",
    "classifier = make_pipeline(\n",
    "    FeatureSelector(pvalue_threshold),\n",
    "    RandomForestClassifier(),\n",
    ")\n",
    "\n",
    "# We perform 5-fold cross-validation again:\n",
    "tprs, aucs = [], []\n",
    "mean_fpr = np.linspace(0, 1, 101)\n",
    "for train, test in StratifiedKFold(n_splits=5).split(X, y):\n",
    "    classifier.fit(X[train], y[train])\n",
    "    y_pred = classifier.predict(X[test])\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y[test], y_pred)\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    aucs.append(roc_auc_score(y[test], y_pred))\n",
    "    \n",
    "plot_roc(tprs, aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e186459",
   "metadata": {},
   "source": [
    "Play around with the number of informative features for our dataset. How does model performance change when more/fewer features are informative? Why is this the case?\n",
    "\n",
    "Performance of the correct classifier, without information leakage, is now significantly lower. We have better than random performance, in contrast to the previous example, because there is a true signal in the data. Nevertheless, when information leakage occurs, we might think that our classifier is significantly better than it will actually perform on novel data.\n",
    "\n",
    "**The prime directive of machine learning:**\n",
    "\n",
    "> Only evaluate your models on data that has never been used for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
