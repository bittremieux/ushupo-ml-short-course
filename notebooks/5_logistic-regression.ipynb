{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9751d648",
   "metadata": {},
   "source": [
    "# Lab 5: Using logistic regression to predict the quality of mass spectrometry experiments.\n",
    "\n",
    "As we learned in the previous lecture, logistic regression is a supervised learning method for classification when you have two classes. In this lab, we're going to explore one use case of logistic regression applied to mass spectrometry data: predicting the quality of an LC-MS experiment. In fact, we'll actually be recreating some of the work presented by [Amidan et al](https://pubs.acs.org/doi/10.1021/pr401143e).\n",
    "\n",
    "Before we begin our analysis though, we need to first import a few Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9304ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                # For working with tabular data.\n",
    "import matplotlib.pyplot as plt    # For plotting data\n",
    "import seaborn as sns              # For theming our plots.\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,            # To use logistic regression.\n",
    "    LogisticRegressionCV,          # Automatically select hyperparameters using cross-validation\n",
    ") \n",
    "from sklearn.preprocessing import StandardScaler # Used to normalize the features\n",
    "\n",
    "# These will calculate our ROC and precision-recall curve metrics.\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    RocCurveDisplay,\n",
    "    average_precision_score,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "\n",
    "# Make our plots look nice:\n",
    "sns.set(context=\"notebook\", style=\"ticks\")\n",
    "\n",
    "# These variables define the paths to our data:\n",
    "metadata_csv = \"../data/quality/metadata.csv\"  # A summary of the data\n",
    "train_csv = \"../data/quality/training.csv\"     # The training set\n",
    "test_csv = \"../data/quality/test.csv\"          # The test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dd81d",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "The data that we'll be working with was manually annotated to label LC-MS datasets on several Orbitrap instruments as either good or poor. Here is how [Amidan et al](https://pubs.acs.org/doi/10.1021/pr401143e) describe the annotation process:\n",
    "> The data sets were manually reviewed by three expert instrument operators (30+ years of combined LC–MS experience) using an in-house graphical user interface viewer. This viewer contained the base peak chromatogram, total ion current chromatogram, plots of both the top 50 000 and top 500 000 LC–MS detected features, and the number of peptides identified. In the first round, 1150 data sets were manually curated as “good”, “okay”, or “poor” and used to develop the classifier. In cases where the assessors disagreed (∼5–10%), the majority opinion was taken for the curated value. Moreover, the “okay” value was used to denote the wide range of performance, which, although not optimal, was still acceptable. \n",
    "\n",
    "This annoation process resulted in the following data thate we'll be using. Run the code cell below to see an overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and display the metadata table:\n",
    "pd.read_csv(metadata_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070010b",
   "metadata": {},
   "source": [
    "Four our model, we're going to focus on the Velo Orbitrap data, because that has the most examples. We must model them each individually, because they have different features. Let's load the training data and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313025f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv) # Read the training data.\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80a8f4",
   "metadata": {},
   "source": [
    "Notice the columns with `NaN` features. These indicate features that are missing for a particular instrument model. Now we'll filter the data for the Velos Orbitrap data and select the feature columns in the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only Velos Orbitrap data:\n",
    "train_velos = train_df.loc[train_df[\"Instrument_Category\"] == \"VOrbitrap\", :]\n",
    "\n",
    "# These columns are not features, so we want to ignore them:\n",
    "not_feature_columns = [\n",
    "    \"Instrument_Category\",\n",
    "    \"Instrument\",\n",
    "    \"Dataset_ID\",\n",
    "    \"Acq_Time_Start\",\n",
    "    \"Acq_Length\",\n",
    "    \"Dataset\",\n",
    "    \"Dataset_Type\",\n",
    "    \"Curated_Quality\",\n",
    "    \"BinomResp\",\n",
    "    \"LLRC.Prediction\",\n",
    "    # These features don't exist for this instrument:\n",
    "    \"MS1_TIC_Q2\",\n",
    "    \"MS1_TIC_Q3\",\n",
    "    \"C_1A\",\n",
    "    \"C_1B\",\n",
    "    \"C_2B\",\n",
    "    \"C_3B\",\n",
    "    \"C_4A\",\n",
    "    \"C_4C\",\n",
    "    \"P_2Anorm\",\n",
    "]\n",
    "\n",
    "# Find the feature columns in the data:\n",
    "# This is a Python feature called a \"list comprehension\"\n",
    "feature_columns = [c for c in train_velos.columns if c not in not_feature_columns]\n",
    "\n",
    "# Extract the features and labels:\n",
    "X_train = train_velos.loc[:, feature_columns]\n",
    "y_train = train_velos.loc[:, \"BinomResp\"]\n",
    "\n",
    "# Print the size of our dataset:\n",
    "print(\"We have\", X_train.shape[1], \"features\")\n",
    "print(\"and\", X_train.shape[0], \"examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d8924",
   "metadata": {},
   "source": [
    "Note that here is where we would normally perform data exploration, making plots of everything we think might be relevant. We're not going to do that here for the sake of time.\n",
    "\n",
    "## Normalize the features\n",
    "\n",
    "It is important to scale (transform all of the features so that they are of the same magnitude) and center (transform all of the features so that their mean is 0) the features of our dataset in order for many machine learning methods to work optimally. This can be easily accomplished for a feature by subtracting the mean and dividing by the standard deviation. This is often called *standardization* or *standard scaling*, because our features will now be similar to a standard normal distribution–they will have a standar deviation of 1 and mean of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eef223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features:\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4caf2",
   "metadata": {},
   "source": [
    "## Fit a logistic regression model\n",
    "\n",
    "We're now ready to fit our logistic regression model. Just as in the paper, we're going to use L1 regularization to perform implicit feature selection—recall that L1 regularization forces small coefficients to go to 0, essentially removing unimportant features from the model. We'll use 3-fold cross-validation to select how much L1 regularization we need, which is a hyperparameter we need to select.\n",
    "\n",
    "Fortunately, the `LogisticRegressionCV` model in scikit-learn can take care of the cross-validation for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e7395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model:\n",
    "model = LogisticRegressionCV(\n",
    "    penalty=\"l1\",       # Use L1 regularization\n",
    "    cv=3,               # Use 3 cross-validation folds.\n",
    "    scoring=\"roc_auc\",  # Use ROC AUC to choose which model is best.\n",
    "    solver=\"saga\",      # Needed to use L1 regularization.\n",
    "    max_iter=5000,      # Needed for the models to finish training.\n",
    ")\n",
    "\n",
    "# Fit the model:\n",
    "model.fit(X_standardized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8459cba",
   "metadata": {},
   "source": [
    "## Let's see how we did\n",
    "\n",
    "Now that we're all done with the modeling process, it's time to assess the performance of the model and look at what the it learned. Since we're done with modeling, it is time to load and predict on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_csv) # Read the test data.\n",
    "\n",
    "# Filter for only Velos Orbitrap data:\n",
    "test_velos = train_df.loc[train_df[\"Instrument_Category\"] == \"VOrbitrap\", :]\n",
    "\n",
    "# Extract the features and labels:\n",
    "X_test = test_velos.loc[:, feature_columns]\n",
    "y_test = test_velos.loc[:, \"BinomResp\"]\n",
    "\n",
    "# Scale and center our test data:\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "# Get the probabilities predicted for the test set examples:\n",
    "probs = model.predict_proba(X_test_standardized)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413995b0",
   "metadata": {},
   "source": [
    "How do these ROC and precision-recall curves look to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e19ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our blank figure with 2 panels\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot the ROC and PR curves:\n",
    "RocCurveDisplay.from_predictions(y_test, probs, ax=axs[0])\n",
    "PrecisionRecallDisplay.from_predictions(y_test, probs, ax=axs[1])\n",
    "\n",
    "# Display the plot:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2fdb9",
   "metadata": {},
   "source": [
    "**Homework:** How do our results compare to that presented in the paper?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:envs]",
   "language": "python",
   "name": "conda-env-envs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
