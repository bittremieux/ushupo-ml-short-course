{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9175fca",
   "metadata": {},
   "source": [
    "# Lab 3: Clustering\n",
    "\n",
    "Clustering is a very common _unsupervised_ machine learning technique. This means that during clustering we do not have class label information, but instead try to find observations that share similar characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e71dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_wine(return_X_y=True, as_frame=True)\n",
    "X_standardized = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea741ce",
   "metadata": {},
   "source": [
    "We will use a wine dataset that contains the results of a chemical analysis of wines grown in a specific area of Italy.\n",
    "\n",
    "Let's explore what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb61ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_col = 3\n",
    "n_row = math.ceil(len(X.columns) / n_col)\n",
    "\n",
    "fig, axes = plt.subplots(n_row, n_col, figsize=(10, 10))\n",
    "\n",
    "for col, ax in zip(X.columns, axes.ravel()):\n",
    "    sns.histplot(data=X, x=col, ax=ax)\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7fbce",
   "metadata": {},
   "source": [
    "Looks good! There are 13 different features. We can also use PCA to reduce the data to two dimensions for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17287a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(X_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9f452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "X_reduced = pca.transform(X_standardized)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1])\n",
    "ax.set_xlabel(\"PCA1\")\n",
    "ax.set_ylabel(\"PCA2\")\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b76164",
   "metadata": {},
   "source": [
    "## k-Means\n",
    "\n",
    "**Now let's do some clustering.** We will use k-means clustering with $k = 5$ initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, n_init=\"auto\", random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec34a0",
   "metadata": {},
   "source": [
    "Let's show the cluster assignments on the PCA plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit_predict(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "X_reduced = pca.transform(X_standardized)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=kmeans.labels_)\n",
    "\n",
    "ax.set_xlabel(\"PCA1\")\n",
    "ax.set_ylabel(\"PCA2\")\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71339c1d",
   "metadata": {},
   "source": [
    "This looks very weird! The cluster assignments don't correspond to our intuition.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><b>What went wrong?</b> (Click to expand)</summary>\n",
    "    \n",
    "**We didn't standardize our data prior to k-Means clustering!** Because our features are on different scales, features with bigger magnitudes dominate the Euclidean distance calculations and k-Means won't give the expected result.\n",
    "\n",
    "_Note:_ We sneakily did standardize our features prior to the PCA transformation as we have seen during the previous lecture. If PCA would have been performed on non-standardized data as well (which is incorrect), the clusters on the plot would look more natural.\n",
    "\n",
    "Let's try this again, but now standardize our features prior to k-Means clustering!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X_standardized)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "X_reduced = pca.transform(X_standardized)\n",
    "centroids = pca.transform(kmeans.cluster_centers_)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=kmeans.labels_)\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], s=100, c=\"red\", marker=\"X\")\n",
    "\n",
    "ax.set_xlabel(\"PCA1\")\n",
    "ax.set_ylabel(\"PCA2\")\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbed52",
   "metadata": {},
   "source": [
    "This makes a lot more sense!\n",
    "<br><br>\n",
    "<details>\n",
    "<summary>\n",
    "<b>Why does there still seem to be a mismatch between the cluster centroids and some of the cluster labels?</b> (Click to expand)\n",
    "</summary>\n",
    "\n",
    "We performed k-Means clustering in the original, 13-dimensional space, whereas plotting happens in two dimensions. PCA causes some loss of information, which makes it seem as if some cluster assignments are incorrect.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60a228",
   "metadata": {},
   "source": [
    "We managed to successfully cluster, but is five clusters the optimal value?\n",
    "<details>\n",
    "<summary>\n",
    "<b>How do we determine the number of clusters?</b> (Click to expand)\n",
    "</summary>\n",
    "\n",
    "We can use the **elbow method** to figure out how many clusters we need. Let's run k-Means for $k=2$ to $k=10$ and plot the cost function (sum of squared distances) for all clusterings.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = [\n",
    "    KMeans(n_clusters=k, n_init=\"auto\", random_state=0).fit(X_reduced).inertia_\n",
    "    for k in range(2, 11)\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(range(2, 11), cost, marker=\"o\")\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"Cost function\")\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8f9f0",
   "metadata": {},
   "source": [
    "Now let's run clustering again with your optimal value of $k$ and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of clusters here:\n",
    "n_clusters = 3# TODO\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\", random_state=0).fit(X_standardized)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "X_reduced = pca.transform(X_standardized)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=kmeans.labels_)\n",
    "\n",
    "ax.set_xlabel(\"PCA1\")\n",
    "ax.set_ylabel(\"PCA2\")\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911579b5",
   "metadata": {},
   "source": [
    "That looks like excellent clustering! We managed to find the optimal number of clusters using the elbow method. For this toy example, a good value was relatively straightforward to determine, but for more complex data a definitive answer might not always be available. Additionally, to use the elbow method, we had to run k-Means for multiple values of $k$, leading to runtime overhead.\n",
    "\n",
    "Because we know the ground truth for this dataset, we do know class labels in this case. _This is normally not the case when using clustering!_ Let's compare our clustering results to the ground truth labeling.\n",
    "\n",
    "We see indeed that there are three classes in our data, which k-Means was able to recover!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3893cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "X_reduced = pca.transform(X_standardized)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)\n",
    "\n",
    "ax.set_xlabel(\"PCA1\")\n",
    "ax.set_ylabel(\"PCA2\")\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9e545",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "Now we'll perform hierarchical clustering on the same data and plot the dendrogram. **Can you derive the optimal number of clusters from the dendrogram?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9667f",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cluster = AgglomerativeClustering(\n",
    "    distance_threshold=0, n_clusters=None, linkage=\"ward\"\n",
    ")\n",
    "\n",
    "agg_cluster.fit(X_standardized)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "plot_dendrogram(agg_cluster, truncate_mode=\"level\", p=100)\n",
    "\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xlabel(\"samples\")\n",
    "ax.set_ylabel(\"Cluster distance\")\n",
    "\n",
    "sns.despine(ax=ax)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe177d",
   "metadata": {},
   "source": [
    "Scikit-learn uses **Ward linkage** by default (not discussed during class), which minimizes the variance of clusters being merged.\n",
    "\n",
    "**Try different values for the linkage method and discuss how this changes the dendrogram.** Try the following linkage methods:\n",
    "\n",
    "- average\n",
    "- complete\n",
    "- single"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
